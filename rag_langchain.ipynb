{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG with LangChain + Gemini + Pinecone\n",
    "\n",
    "Este notebook implementa un sistema RAG usando LangChain, Gemini y Pinecone donde primero se carga la documentación oficial de LangChain, luego se divide en fragmentos pequeños para poder trabajarla mejor, después se generan embeddings con Google Gemini para convertir el texto en vectores numéricos, esos vectores se almacenan en una base de datos vectorial en Pinecone para poder hacer búsquedas semánticas, cuando el usuario hace una pregunta esta se transforma también en embedding, se consulta Pinecone para traer los fragmentos más relevantes según similitud, y finalmente esos fragmentos junto con la pregunta se envían al modelo Gemini con un prompt adecuado para generar una respuesta final mucho más precisa y contextualizada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d57e2534",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain-text-splitters 0.3.5 requires langchain-core<0.4.0,>=0.3.29, but you have langchain-core 1.2.15 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q \\\n",
    "    langchain \\\n",
    "    langchain-google-genai \\\n",
    "    langchain-pinecone \\\n",
    "    langchain-community \\\n",
    "    pinecone-client \\\n",
    "    python-dotenv \\\n",
    "    bs4 \\\n",
    "    tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a34555",
   "metadata": {},
   "source": [
    "## 2. Load API Keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6146076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys loaded successfully\n",
      "Pinecone index name: rag-langchain-docs\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(dotenv_path=\"../.env\")\n",
    "\n",
    "GOOGLE_API_KEY     = os.getenv(\"GOOGLE_API_KEY\")\n",
    "PINECONE_API_KEY   = os.getenv(\"PINECONE_API_KEY\")\n",
    "PINECONE_INDEX     = os.getenv(\"PINECONE_INDEX_NAME\", \"rag-langchain-docs\")\n",
    "\n",
    "assert GOOGLE_API_KEY,   \"Missing GOOGLE_API_KEY\"\n",
    "assert PINECONE_API_KEY, \"Missing PINECONE_API_KEY\"\n",
    "\n",
    "print(\"API Keys loaded successfully\")\n",
    "print(f\"Pinecone index name: {PINECONE_INDEX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4690deb",
   "metadata": {},
   "source": [
    "## 3. Load & Split Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3be57432",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "langchain 1.2.10 requires langchain-core<2.0.0,>=1.2.10, but you have langchain-core 0.3.83 which is incompatible.\n",
      "langchain-openai 1.1.10 requires langchain-core<2.0.0,>=1.2.13, but you have langchain-core 0.3.83 which is incompatible.\n",
      "langgraph-prebuilt 1.0.8 requires langchain-core>=1.0.0, but you have langchain-core 0.3.83 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d6b6b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from LangChain docs...\n",
      "Loaded 3 documents\n",
      "Total characters: 39,750\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "os.environ[\"USER_AGENT\"] = \"RAG-LangChain-Lab/1.0\"\n",
    "\n",
    "URLS = [\n",
    "    \"https://python.langchain.com/docs/tutorials/rag/\",\n",
    "    \"https://python.langchain.com/docs/concepts/rag/\",\n",
    "    \"https://python.langchain.com/docs/concepts/vectorstores/\",\n",
    "]\n",
    "\n",
    "print(\"Loading documents from LangChain docs...\")\n",
    "\n",
    "loader = WebBaseLoader(web_paths=URLS)\n",
    "docs = loader.load()\n",
    "\n",
    "docs = [d for d in docs if len(d.page_content.strip()) > 100]\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents\")\n",
    "print(f\"Total characters: {sum(len(d.page_content) for d in docs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5723d565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split into 58 chunks\n",
      "\n",
      " Example chunk (first 300 chars):\n",
      "--------------------------------------------------\n",
      "Build a RAG agent with LangChain - Docs by LangChainSkip to main contentDocs by LangChain home pageOpen sourceSearch...⌘KAsk AIGitHubTry LangSmithTry LangSmithSearch...NavigationLangChainBuild a RAG agent with LangChainDeep AgentsLangChainLangGraphIntegrationsLearnReferenceContributePythonLearnTutor\n",
      "--------------------------------------------------\n",
      "\n",
      "Metadata: {'source': 'https://python.langchain.com/docs/tutorials/rag/', 'title': 'Build a RAG agent with LangChain - Docs by LangChain', 'language': 'en', 'start_index': 0}\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    add_start_index=True\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split into {len(splits)} chunks\")\n",
    "print(f\"\\n Example chunk (first 300 chars):\")\n",
    "print(\"-\" * 50)\n",
    "print(splits[0].page_content[:300])\n",
    "print(\"-\" * 50)\n",
    "print(f\"\\nMetadata: {splits[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setup Embeddings (Google Gemini)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(\n",
    "    model=\"models/text-embedding-004\", \n",
    "    google_api_key=GOOGLE_API_KEY\n",
    ")\n",
    "\n",
    "test_vector = embeddings.embed_query(\"What is LangChain?\")\n",
    "print(f\"Embedding model ready\")\n",
    "print(f\"Embedding dimensions: {len(test_vector)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Setup Pinecone & Index Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "import time\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "existing_indexes = [idx.name for idx in pc.list_indexes()]\n",
    "print(f\"Existing Pinecone indexes: {existing_indexes}\")\n",
    "\n",
    "if PINECONE_INDEX not in existing_indexes:\n",
    "    print(f\"Creating new index: '{PINECONE_INDEX}'...\")\n",
    "    pc.create_index(\n",
    "        name=PINECONE_INDEX,\n",
    "        dimension=768,         \n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\" \n",
    "        )\n",
    "    )\n",
    "\n",
    "    while not pc.describe_index(PINECONE_INDEX).status[\"ready\"]:\n",
    "        print(\"Waiting for index to be ready...\")\n",
    "        time.sleep(2)\n",
    "    print(f\"Index '{PINECONE_INDEX}' created!\")\n",
    "else:\n",
    "    print(f\"Index '{PINECONE_INDEX}' already exists, reusing it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "index = pc.Index(PINECONE_INDEX)\n",
    "stats = index.describe_index_stats()\n",
    "vector_count = stats.get(\"total_vector_count\", 0)\n",
    "\n",
    "print(f\"Vectors currently in index: {vector_count}\")\n",
    "\n",
    "if vector_count == 0:\n",
    "    print(f\"⬆Uploading {len(splits)} chunks to Pinecone...\")\n",
    "    vectorstore = PineconeVectorStore.from_documents(\n",
    "        documents=splits,\n",
    "        embedding=embeddings,\n",
    "        index_name=PINECONE_INDEX,\n",
    "        pinecone_api_key=PINECONE_API_KEY\n",
    "    )\n",
    "    print(f\"Documents indexed successfully!\")\n",
    "else:\n",
    "    print(\"Connecting to existing vectorstore (skipping upload)...\")\n",
    "    vectorstore = PineconeVectorStore(\n",
    "        index_name=PINECONE_INDEX,\n",
    "        embedding=embeddings,\n",
    "        pinecone_api_key=PINECONE_API_KEY\n",
    "    )\n",
    "    print(\"Connected to existing vectorstore!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Setup Retriever\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 4}  \n",
    ")\n",
    "\n",
    "\n",
    "test_query = \"What is a vector store in LangChain?\"\n",
    "retrieved_docs = retriever.invoke(test_query)\n",
    "\n",
    "print(f\"Query: '{test_query}'\")\n",
    "print(f\"Retrieved {len(retrieved_docs)} relevant chunks:\\n\")\n",
    "\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"--- Chunk {i+1} (from: {doc.metadata.get('source', 'unknown')}) ---\")\n",
    "    print(doc.page_content[:200])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Setup LLM (Gemini)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    google_api_key=GOOGLE_API_KEY,\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "response = llm.invoke(\"Say 'RAG system ready!' in one sentence.\")\n",
    "print(f\"LM test: {response.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build the RAG Chain\n",
    "\n",
    "Aquí combinamos tres cosas clave, primero un prompt personalizado que le dice a Gemini que responda usando únicamente el contexto recuperado y no información inventada, segundo el retriever que se encarga de buscar en la base vectorial los fragmentos más relevantes según la pregunta del usuario, y tercero el modelo LLM que toma esos fragmentos junto con la pregunta y genera la respuesta final de forma coherente y contextualizada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "\n",
    "RAG_PROMPT = \"\"\"\\\n",
    "You are an expert assistant on LangChain documentation.\n",
    "Use ONLY the following retrieved context to answer the question.\n",
    "If you cannot find the answer in the context, say \"I don't have enough context to answer this.\"\n",
    "Keep your answer concise and accurate.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(RAG_PROMPT)\n",
    "\n",
    "def format_docs(docs):\n",
    "    \"\"\"Concatenate retrieved chunks into a single context string.\"\"\"\n",
    "    return \"\\n\\n\".join(\n",
    "        f\"[Source: {doc.metadata.get('source', 'unknown')}]\\n{doc.page_content}\"\n",
    "        for doc in docs\n",
    "    )\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"RAG Chain built successfully!\")\n",
    "print(\"\\nChain structure:\")\n",
    "print(\"   User Question\")\n",
    "print(\"   Retriever (Pinecone similarity search)\")\n",
    "print(\"   Prompt Template\")\n",
    "print(\"   Gemini 1.5 Flash (LLM)\")\n",
    "print(\"   Final Answer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Demo — Ask Questions!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question: str):\n",
    "    \"\"\"Ask the RAG system a question and display the result.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"Answer:\\n{answer}\")\n",
    "    return answer\n",
    "\n",
    "questions = [\n",
    "    \"What is RAG and why is it useful?\",\n",
    "    \"What is a vector store and how does it work in LangChain?\",\n",
    "    \"How does the retrieval step work in a RAG pipeline?\",\n",
    "    \"What is LCEL (LangChain Expression Language)?\",\n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    ask(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Interactive Mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "my_question = \"How do I add memory to a RAG chain?\"\n",
    "\n",
    "ask(my_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Inspect Retrieved Context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_with_sources(question: str):\n",
    "    \"\"\"Ask a question and show both the answer and the source chunks.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    \n",
    "    print(f\"\\nRetrieved {len(relevant_docs)} source chunks:\")\n",
    "    for i, doc in enumerate(relevant_docs):\n",
    "        print(f\"\\n  [{i+1}] Source: {doc.metadata.get('source', 'unknown')}\")\n",
    "        print(f\"       Preview: {doc.page_content[:150]}...\")\n",
    "    \n",
    "    answer = rag_chain.invoke(question)\n",
    "    print(f\"\\n Answer:\\n{answer}\")\n",
    "\n",
    "ask_with_sources(\"What components make up a RAG system?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
